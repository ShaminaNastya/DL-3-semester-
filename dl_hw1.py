# -*- coding: utf-8 -*-
"""DL-HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EOueSYWooSxq706AMY3tmQ9lRHIdwEIk
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

import torch
from torch import nn

import pandas as pd
import numpy as np
import math as math
import matplotlib.pyplot as plt
import missingno as msno
import seaborn as sns
from pandas.api.types import is_string_dtype
from pandas.api.types import is_numeric_dtype
from IPython.display import Markdown, display
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import json

import warnings
warnings.filterwarnings('ignore')

import random
# Set seed for reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# ***Предобработка данных***"""

train_df = pd.read_csv("loan_train.csv")
test_df = pd.read_csv("loan_test.csv")

train_df

test_df

def list_column_values(df, number_of_values, print_all):
    display(Markdown('**Results:**' ))
    for col in df.columns[0:]:
        if df[col].nunique() <= number_of_values:
            print(f"{col.ljust(25)}" +  ' ==> ' + str(df[col].sort_values().unique().tolist()) )
        else:
            if print_all=='True':
               print(f"{col.ljust(25)}" + ' ==> more than ' + str(number_of_values) + ' values')

list_column_values(train_df,15,'False')

train = train_df

train.columns

train['person_home_ownership'].replace('MORTGAGE', 0, inplace=True)
train['person_home_ownership'].replace('OTHER', 1, inplace=True)
train['person_home_ownership'].replace('OWN', 2, inplace=True)
train['person_home_ownership'].replace('RENT', 3, inplace=True)

train['loan_intent'].replace('DEBTCONSOLIDATION', 0, inplace=True)
train['loan_intent'].replace('EDUCATION', 1, inplace=True)
train['loan_intent'].replace('HOMEIMPROVEMENT', 2, inplace=True)
train['loan_intent'].replace('MEDICAL', 3, inplace=True)
train['loan_intent'].replace('PERSONAL', 4, inplace=True)
train['loan_intent'].replace('VENTURE', 5, inplace=True)

train['loan_grade'].replace('A', 0, inplace=True)
train['loan_grade'].replace('B', 1, inplace=True)
train['loan_grade'].replace('C', 2, inplace=True)
train['loan_grade'].replace('D', 3, inplace=True)
train['loan_grade'].replace('E', 4, inplace=True)
train['loan_grade'].replace('F', 5, inplace=True)
train['loan_grade'].replace('G', 6, inplace=True)

train['cb_person_default_on_file'].replace('N', 0, inplace=True)
train['cb_person_default_on_file'].replace('Y', 1, inplace=True)

train = train.drop('id', axis=1)

train

train.isna().sum()

y = train['loan_status']

x = train.drop('loan_status', axis=1)

from sklearn.preprocessing import StandardScaler

std = StandardScaler()  #Произведём масштабирование данных с помощью стандартизации (вычитание матожидания плюс деление на ско)

x_std = std.fit_transform(x)

x_std = pd.DataFrame(x_std, columns=x.columns)
x_std.describe()

X_train = x_std
y_train = y

test = test_df

test['person_home_ownership'].replace('MORTGAGE', 0, inplace=True)
test['person_home_ownership'].replace('OTHER', 1, inplace=True)
test['person_home_ownership'].replace('OWN', 2, inplace=True)
test['person_home_ownership'].replace('RENT', 3, inplace=True)

test['loan_intent'].replace('DEBTCONSOLIDATION', 0, inplace=True)
test['loan_intent'].replace('EDUCATION', 1, inplace=True)
test['loan_intent'].replace('HOMEIMPROVEMENT', 2, inplace=True)
test['loan_intent'].replace('MEDICAL', 3, inplace=True)
test['loan_intent'].replace('PERSONAL', 4, inplace=True)
test['loan_intent'].replace('VENTURE', 5, inplace=True)

test['loan_grade'].replace('A', 0, inplace=True)
test['loan_grade'].replace('B', 1, inplace=True)
test['loan_grade'].replace('C', 2, inplace=True)
test['loan_grade'].replace('D', 3, inplace=True)
test['loan_grade'].replace('E', 4, inplace=True)
test['loan_grade'].replace('F', 5, inplace=True)
test['loan_grade'].replace('G', 6, inplace=True)

test['cb_person_default_on_file'].replace('N', 0, inplace=True)
test['cb_person_default_on_file'].replace('Y', 1, inplace=True)

y_test = test['loan_status']

x_test = test.drop('loan_status', axis=1)

xtest_std = std.transform(x)

xtest_std = pd.DataFrame(x_std, columns=x.columns)

X_valid = x_std
y_valid = y

X_train = X_train.values
X_valid = X_valid.values
y_train = y_train.values
y_valid = y_valid.values

X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train).view(-1, 1)
X_valid_tensor = torch.FloatTensor(X_valid)
y_valid_tensor = torch.FloatTensor(y_valid).view(-1, 1)

from torch import optim
from sklearn.metrics import roc_auc_score, roc_curve

"""# ***Задание №1***"""

# Model №1
class SimpleModel1(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleModel1, self).__init__()
        # 1. Все входные данные в hidden_size
        self.linear1 = nn.Linear(input_size, hidden_size)

        # 2. Простой блок
        self.linear2 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu = nn.ReLU()
        self.linear3 = nn.Linear(hidden_size * 4, hidden_size)

        # 3. Linear (hidden size в выход)
        self.output = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.linear1(x)
        x = self.linear2(x)
        x = self.relu(x)
        x = self.linear3(x)
        x = self.output(x)
        x = self.sigmoid(x)
        return x

input_size = X_train.shape[1]
hidden_size = 32
model = SimpleModel1(input_size, hidden_size)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Графики немного напоминают даже линейную функцию (разумеется, немного выгнутую)
- Решила учить 100 эпох. Возможно, это много, но Roc AUC немного подрастает.

--------------------------------------------------------------------------------

# ***Задание №2***
"""

# Теперь hidden_size = 128

# Model №2
class SimpleModel2(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleModel2, self).__init__()

        # 1. Все входные данные в hidden_size
        self.linear1 = nn.Linear(input_size, hidden_size)

        # 2. Первый простой блок
        self.linear2 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu1 = nn.ReLU()
        self.linear3 = nn.Linear(hidden_size * 4, hidden_size)

        # 3. Второй блок
        self.linear4 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu2 = nn.ReLU()
        self.linear5 = nn.Linear(hidden_size * 4, hidden_size)

        # 4. Третий блок
        self.linear6 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu3 = nn.ReLU()
        self.linear7 = nn.Linear(hidden_size * 4, hidden_size)

        # 5. Linear (hidden size в выход)
        self.output = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.linear1(x)
        # Первый простой блок
        x = self.linear2(x)
        x = self.relu1(x)
        x = self.linear3(x)
        # Второй блок
        x = self.linear4(x)
        x = self.relu2(x)
        x = self.linear5(x)
        # Третий блок
        x = self.linear6(x)
        x = self.relu3(x)
        x = self.linear7(x)
        # Выход
        x = self.output(x)
        x = self.sigmoid(x)
        return x

input_size = X_train.shape[1]
hidden_size = 128
model = SimpleModel2(input_size, hidden_size)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- График Roc AUC заметно стал выпуклым
- По сравнению с предыдущим экспериментом заметны улучшения показателей Roc AUC на валиде и трейне, небольшое увеличение лосса (на обоих выборках).

--------------------------------------------------------------------------------

# ***Задание №3***
"""

# Теперь hidden_size = 128

# Model №3
class SimpleModel3(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleModel3, self).__init__()

        # 1. Все входные данные в hidden_size
        self.linear1 = nn.Linear(input_size, hidden_size)

        # 2. Первый простой блок
        self.bn1 = nn.BatchNorm1d(hidden_size) # Batch Norm 1
        self.linear2 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu1 = nn.ReLU()
        self.linear3 = nn.Linear(hidden_size * 4, hidden_size)

        # 3. Второй блок
        self.bn2 = nn.BatchNorm1d(hidden_size) # Batch Norm 2
        self.linear4 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu2 = nn.ReLU()
        self.linear5 = nn.Linear(hidden_size * 4, hidden_size)

        # 4. Третий блок
        self.bn3 = nn.BatchNorm1d(hidden_size) # Batch Norm 3
        self.linear6 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu3 = nn.ReLU()
        self.linear7 = nn.Linear(hidden_size * 4, hidden_size)

        # 5. Linear (hidden size в выход)
        self.output = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.linear1(x)

        # Первый простой блок
        identity1 = x  # Skip connection 1
        x = self.bn1(x)
        x = self.linear2(x)
        x = self.relu1(x)
        x = self.linear3(x)
        x = x + identity1 # Skip connection 1

        # Второй блок
        identity2 = x  # Skip connection 2
        x = self.bn2(x)
        x = self.linear4(x)
        x = self.relu2(x)
        x = self.linear5(x)
        x = x + identity2 # Skip connection 2

        # Третий блок
        identity3 = x  # Skip connection 3
        x = self.bn3(x)
        x = self.linear6(x)
        x = self.relu3(x)
        x = self.linear7(x)
        x = x + identity3 # Skip connection 3

        # Выход
        x = self.output(x)
        x = self.sigmoid(x)
        return x

input_size = X_train.shape[1]
hidden_size = 128
model = SimpleModel3(input_size, hidden_size)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Всё сразу подросло.
-Лосс в итоге заметно уменьшился, точность немного выросла, Roc AUC увеличился (по сравнению с предыдущей задачей)

--------------------------------------------------------------------------------

# ***Задание №4***
"""

# Model №4
class SimpleModel4(nn.Module):
    def __init__(self, input_size, hidden_size, dropout_p=0.0):
        super(SimpleModel4, self).__init__()
        self.dropout_p = dropout_p

        # 1. Все входные данные в hidden_size
        self.linear1 = nn.Linear(input_size, hidden_size)

        # 2. Первый простой блок
        self.bn1 = nn.BatchNorm1d(hidden_size) # Batch Norm 1
        self.linear2 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout_p) # Dropout 1
        self.linear3 = nn.Linear(hidden_size * 4, hidden_size)

        # 3. Второй блок
        self.bn2 = nn.BatchNorm1d(hidden_size) # Batch Norm 2
        self.linear4 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout_p) # Dropout 2
        self.linear5 = nn.Linear(hidden_size * 4, hidden_size)

        # 4. Третий блок
        self.bn3 = nn.BatchNorm1d(hidden_size) # Batch Norm 3
        self.linear6 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu3 = nn.ReLU()
        self.dropout3 = nn.Dropout(dropout_p) # Dropout 3
        self.linear7 = nn.Linear(hidden_size * 4, hidden_size)

        # 5. Linear (hidden size в выход)
        self.output = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.linear1(x)

        # Первый простой блок
        identity1 = x  # Skip connection 1
        x = self.bn1(x)
        x = self.linear2(x)
        x = self.relu1(x)
        x = self.dropout1(x) # Apply dropout 2
        x = self.linear3(x)
        x = x + identity1 # Skip connection 1

        # Второй блок
        identity2 = x  # Skip connection 2
        x = self.bn2(x)
        x = self.linear4(x)
        x = self.relu2(x)
        x = self.dropout2(x) # Apply dropout 2
        x = self.linear5(x)
        x = x + identity2 # Skip connection 2

        # Третий блок
        identity3 = x  # Skip connection 3
        x = self.bn3(x)
        x = self.linear6(x)
        x = self.relu3(x)
        x = self.dropout3(x) # Apply dropout 3
        x = self.linear7(x)
        x = x + identity3 # Skip connection 3

        # Выход
        x = self.output(x)
        x = self.sigmoid(x)
        return x

# Possible Dropout values
DROPOUT_VALUES = [0.01, 0.1, 0.2, 0.5, 0.9]
input_size = X_train.shape[1]
hidden_size = 128
drop = 0.01
model = SimpleModel4(input_size, hidden_size, drop)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Результаты немного лучше, нежели без дропаута

---------------------------------------------------------------------------
"""

input_size = X_train.shape[1]
hidden_size = 128
drop = 0.1
model = SimpleModel4(input_size, hidden_size, drop)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Roc AUC немного увеличивается, как и точность.

---------------------------------------------------------------------------
"""

input_size = X_train.shape[1]
hidden_size = 128
drop = 0.2
model = SimpleModel4(input_size, hidden_size, drop)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Roc AUC на валиде начинает превосходить трейн и сам он немного увеличился по сравнению с дропаутом в 0.1

- Именно это значение дропаута решаю зафиксировать, как оптимальное.

---------------------------------------------------------------------------
"""

input_size = X_train.shape[1]
hidden_size = 128
drop = 0.5
model = SimpleModel4(input_size, hidden_size, drop)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Уже становится очевидно, что Roc AUC на валиде достаточно превосходит трейн

---------------------------------------------------------------------------
"""

input_size = X_train.shape[1]
hidden_size = 128
drop = 0.9
model = SimpleModel4(input_size, hidden_size, drop)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
num_epochs = 100
train_losses = []
valid_losses = []
train_accuracies = []
valid_accuracies = []
train_roc_aucs = []  # Список для хранения значений ROC AUC на train
valid_roc_aucs = []  # Список для хранения значений ROC AUC на validation

for epoch in range(num_epochs):
    model.train()

    outputs = model(X_train_tensor)
    train_loss = criterion(outputs, y_train_tensor)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        valid_outputs = model(X_valid_tensor)
        valid_loss = criterion(valid_outputs, y_valid_tensor)

        train_predictions = (outputs >= 0.5).float()
        valid_predictions = (valid_outputs >= 0.5).float()
        train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
        valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
        train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())  # Переводим данные на CPU и в NumPy
        valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy()) # Переводим данные на CPU и в NumPy

    # Потери и точность(аккуратность)
    train_losses.append(train_loss.item())
    valid_losses.append(valid_loss.item())
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    train_roc_aucs.append(train_roc_auc)
    valid_roc_aucs.append(valid_roc_auc)

    # Вывод обучения
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
          f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
          f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

plt.figure(figsize=(12, 5))
epochs = range(1, num_epochs + 1)

# Потери
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss',color="green")
plt.plot(valid_losses, label='Valid Loss', color="red")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epochs')
plt.legend()

# График ROC AUC
plt.subplot(1, 2, 2)
plt.plot(epochs, train_roc_aucs, label='Train ROC AUC', color="green")
plt.plot(epochs, valid_roc_aucs, label='Valid ROC AUC', color="red")
plt.title('ROC AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC AUC')
plt.legend()

plt.tight_layout()
plt.show()

"""- Лосс на валиде в любую эпоху заметно меньше лосса на трейне

- Roc AUC на валиде намного больше, чем на трейне

0.2 - dropout оптимальный

--------------------------------------------------------------------------------

# ***Задание №5***
"""

# Model №4
class SimpleModel4(nn.Module):
    def __init__(self, input_size, hidden_size, dropout_p=0.0):
        super(SimpleModel4, self).__init__()
        self.dropout_p = dropout_p

        # 1. Все входные данные в hidden_size
        self.linear1 = nn.Linear(input_size, hidden_size)

        # 2. Первый простой блок
        self.bn1 = nn.BatchNorm1d(hidden_size) # Batch Norm 1
        self.linear2 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout_p) # Dropout 1
        self.linear3 = nn.Linear(hidden_size * 4, hidden_size)

        # 3. Второй блок
        self.bn2 = nn.BatchNorm1d(hidden_size) # Batch Norm 2
        self.linear4 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout_p) # Dropout 2
        self.linear5 = nn.Linear(hidden_size * 4, hidden_size)

        # 4. Третий блок
        self.bn3 = nn.BatchNorm1d(hidden_size) # Batch Norm 3
        self.linear6 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu3 = nn.ReLU()
        self.dropout3 = nn.Dropout(dropout_p) # Dropout 3
        self.linear7 = nn.Linear(hidden_size * 4, hidden_size)

        # 5. Linear (hidden size в выход)
        self.output = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.linear1(x)

        # Первый простой блок
        identity1 = x  # Skip connection 1
        x = self.bn1(x)
        x = self.linear2(x)
        x = self.relu1(x)
        x = self.dropout1(x) # Apply dropout 2
        x = self.linear3(x)
        x = x + identity1 # Skip connection 1

        # Второй блок
        identity2 = x  # Skip connection 2
        x = self.bn2(x)
        x = self.linear4(x)
        x = self.relu2(x)
        x = self.dropout2(x) # Apply dropout 2
        x = self.linear5(x)
        x = x + identity2 # Skip connection 2

        # Третий блок
        identity3 = x  # Skip connection 3
        x = self.bn3(x)
        x = self.linear6(x)
        x = self.relu3(x)
        x = self.dropout3(x) # Apply dropout 3
        x = self.linear7(x)
        x = x + identity3 # Skip connection 3

        # Выход
        x = self.output(x)
        x = self.sigmoid(x)
        return x

def train_and_evaluate(model, X_train_tensor, y_train_tensor, X_valid_tensor, y_valid_tensor, learning_rate, weight_decay, num_epochs):

    criterion = nn.BCELoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    train_losses = []
    valid_losses = []
    train_accuracies = []
    valid_accuracies = []
    train_roc_aucs = []
    valid_roc_aucs = []

    for epoch in range(num_epochs):
        model.train()

        outputs = model(X_train_tensor)
        train_loss = criterion(outputs, y_train_tensor)
        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            valid_outputs = model(X_valid_tensor)
            valid_loss = criterion(valid_outputs, y_valid_tensor)

            train_predictions = (outputs >= 0.5).float()
            valid_predictions = (valid_outputs >= 0.5).float()
            train_accuracy = (train_predictions.eq(y_train_tensor).sum() / y_train_tensor.size(0)).item()
            valid_accuracy = (valid_predictions.eq(y_valid_tensor).sum() / y_valid_tensor.size(0)).item()
            train_roc_auc = roc_auc_score(y_train_tensor.cpu().numpy(), outputs.cpu().numpy())
            valid_roc_auc = roc_auc_score(y_valid_tensor.cpu().numpy(), valid_outputs.cpu().numpy())

        train_losses.append(train_loss.item())
        valid_losses.append(valid_loss.item())
        train_accuracies.append(train_accuracy)
        valid_accuracies.append(valid_accuracy)
        train_roc_aucs.append(train_roc_auc)
        valid_roc_aucs.append(valid_roc_auc)

        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Valid Loss: {valid_loss.item():.4f}, '
              f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
              f'Train ROC AUC: {train_roc_auc:.4f}, Valid ROC AUC: {valid_roc_auc:.4f}')

    return valid_roc_aucs[-1]

weight_decay_values = [0.1, 0.01, 0.001]
learning_rate_values = [0.01, 0.05, 0.1]
num_epochs = 100
drop = 0.2

best_valid_roc_auc = 0.0
best_params = None
input_size = X_train.shape[1]

for weight_decay in weight_decay_values:
    for learning_rate in learning_rate_values:

        print(f"\nTraining with weight_decay={weight_decay}, learning_rate={learning_rate}")
        model = SimpleModel4(input_size, hidden_size=128, dropout_p=drop)
        valid_roc_auc = train_and_evaluate(model, X_train_tensor, y_train_tensor, X_valid_tensor, y_valid_tensor, learning_rate, weight_decay, num_epochs)
        print(f"Final Validation ROC AUC: {valid_roc_auc:.4f}")

        if valid_roc_auc > best_valid_roc_auc:
            best_valid_roc_auc = valid_roc_auc
            best_params = {'weight_decay': weight_decay, 'learning_rate': learning_rate}

print("\nBest Hyperparameters:")
print(best_params)
print(f"Best Validation ROC AUC: {best_valid_roc_auc:.4f}")

"""- Если выбирать самую оптимальную пару значений по показателю Roc AUC на валидационной выборке, то получим weight_decay = 0.001 и learning_rate = 0.1 - именно на них и достигается точность в 0.91 на трейне и 0.92 на валидационной."""